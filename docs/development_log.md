# Development Log — LabelCheck (TTB Label Verification)

## Summary

| Date   | Milestone                                      | Status    |
|--------|-------------------------------------------------|-----------|
| Feb 6  | Project setup, PRD creation, backend scaffolded | Complete  |
| Feb 6  | Multi-agent build of remaining phases           | Complete  |
| Feb 7  | Bug fixes: batch upload, matching, business logic | Complete |
| Feb 8  | Government warning bold detection, demo link    | Complete  |
| Feb 9  | Stakeholder brief, additional test data         | Complete  |
| Feb 10 | Bold detection accuracy, code review remediation | Complete    |

---

## Feb 6 — Project Setup and Initial Build

### Setup (12:00 PM)
- Downloaded the take-home instructions and created the repository.
- Broke the instructions down into markdown for easier reference.
- Used Claude Code to analyze the markdown and generate a PRD (`docs/prd.md`).
- Used the PRD to plan the MVP and select the tech stack.
- Front-loaded `CLAUDE.md` with coding standards, file references, and preferred paradigms.

### Infrastructure
- Set up Azure AI Foundry with GPT-4o mini for LLM analysis.
- Set up Azure Vision Service for OCR.

### Backend Scaffold (Phase 1 Complete)
- Backend structure generated but not yet validated or tested.

### Multi-Agent Build (2:54 PM – 3:44 PM)
- Leveraged Claude Code's experimental Teams feature to complete the remaining project phases.
- Created a team of agents with specialized roles (development, testing, etc.).
- Plan generated by 3:05 PM, agents deployed immediately after.
- Full build reported complete by 3:44 PM (~50 minutes total).

### Testing Pass (4:48 PM)
- Used Claude Code to run feature tests across the app.

---

## Feb 7 — Bug Fixes and Business Logic Corrections

### Bugs Found
- **Batch upload missing:** The app did not support batch label uploads.
- **Matching not functional:** The application-to-label matching feature was not implemented. Root cause: incorrect wording in the original prompt to the LLM.
- **Form display broken:** Matching results were not displayed alongside the application form.

### Decision: Mandatory Application Matching
- Claude Code had made application matching optional. Overrode this — matching is mandatory for the app's purpose.
- In production, applications would come from an existing database. For this demo, users fill out a form as a fallback.

**Takeaway:** AI-assisted coding requires active review and approval of business logic decisions. The assistant optimizes for flexibility by default; domain intent must be enforced by the developer.

### Testing
- Added AI-generated test label images (created with ChatGPT).
- Used Claude Code's multi-agent Teams feature to run UX testing (plan, then implement).

### Performance Issue
- Mock API endpoints were interfering with speed benchmarking.
- Removed all mock endpoints to get accurate measurements. (Trade-off: lost mock coverage for later testing.)

### Business Logic Change
- Original flow: LLM checks first, regex as redundancy.
- Changed to: regex first, LLM only when needed — significantly faster.

---

## Feb 8 — Government Warning Label and Bold Detection

### Problem: Government Warning Search Logic
- The search pattern for the government warning label was incorrect. Fixed the matching logic.
- Realized bold text detection was missing entirely — TTB regulations require the "GOVERNMENT WARNING" header to be bold.

### Bold Detection: Research and Implementation
- **LLM approach:** Accurate but slow (~6 seconds), expensive. Exceeds the 5-second performance budget.
- **OpenCV approach:** Fast, uses stroke width analysis to detect bold text. Concern: typically requires non-bold text as a reference baseline, and there's no reliable anchor for that on every label.
- **Decision:** Tested OpenCV anyway. It worked, but accuracy needs further validation.

### Demo Link
- Set up the demo deployment link.
- Created a setup script for easy local deployment.

---

## Feb 9 — Stakeholder Brief and Additional Testing

### Stakeholder Brief
- Used Claude Code's multi-agent feature to generate a stakeholder brief.
- Brief was written for non-technical stakeholders (not a technical review).

### Additional Test Data
- Added more test label images to improve coverage.
- Added a frontend feature to select from pre-loaded sample images (avoids requiring users to upload during demos).

---

## Feb 10 — Bold Detection Accuracy and Code Review

### Problem: Bold Detection Revisited
- OpenCV bold detection is fast but not accurate enough for compliance purposes.
- Need a solution that is: (1) accurate, (2) within the 5-second window, and (3) cost-effective.
- Fixing flag logic so bold detection results are reported correctly.

### User Testing with Claude Code
- Generated 50+ automated test cases with Claude Code.
- All tests passed — but this was expected to be unreliable. AI-generated tests tend to confirm the happy path without adversarial edge cases.

**Takeaway:** AI assistants execute instructions well but do not independently identify what *should* be tested. The developer must define the test strategy; the assistant implements it.

### Code Review
- Ran a full-scale code review as a precaution.
- Uncovered multiple issues: bugs, security concerns, and code quality problems.
- Documented findings in `docs/CODE_REVIEW.md`.
- Using Claude Code's multi-agent feature to address the review items.

### Code Review Remediation
- Used `docs/CODE_REVIEW.md` as the task list to systematically address all findings.
- Deployed Claude Code's multi-agent Teams feature to fix issues across backend and frontend in parallel.
- All critical and high priority items resolved (`c591e8e`).
- Continued with medium/low items in both backend (`acce154`) and frontend (`dc27bdd`).
- Added `requirements.lock` for reproducible builds as part of security remediation (`326a8b4`).
- Marked 20+ items resolved across two passes of `CODE_REVIEW.md` updates.

---

## Recurring Themes and Lessons Learned

1. **AI assistants are execution tools, not thinking tools.** They do what you tell them. If your instructions are vague or incorrect, the output will be too.
2. **Always review AI-generated business logic.** Defaults will trend toward flexibility and optionality, which may contradict domain requirements.
3. **Mock removal has consequences.** Removing mocks for speed testing improved benchmarking accuracy but lost test isolation for later.
4. **Regex-first, LLM-second** is the right performance pattern for rule-based compliance checks.
5. **Bold detection remains an open problem** — balancing speed, accuracy, and cost for visual text analysis on variable label layouts.
6. **Multi-agent Teams feature** proved effective for parallelizing large build tasks and code review remediation.
